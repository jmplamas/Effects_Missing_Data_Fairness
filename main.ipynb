{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook explores how different missing data patterns and imputation methods influence both predictive performance and fairness across demographic groups. The [Adult Income Dataset](https://www.kaggle.com/datasets/wenruliu/adult-income-dataset), the [COMPAS Recidivism Racial Bias Dataset](https://github.com/propublica/compas-analysis/tree/master) and the [German Credit Risk Dataset](https://www.kaggle.com/datasets/kabure/german-credit-data-with-risk/data) will be used for this analysis.\n",
    "\n",
    "For more details on the work, read the [report](IAS_Individual_Assignment_Report.pdf)!"
   ],
   "id": "5c53a56c4cea3c5d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.calibration import calibration_curve\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import ttest_rel\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clean and Load Adult Income Dataset\n",
    "When cleaning the Adult Income Dataset, make sure to replace \"?\" with NaN values to properly identify missing values. Also, drop unnecessary columns such as \"fnlwgt\" (final weight) and \"educational-num\" (redundant with \"education\"). Finally, standardise the target variable \"income\" to be binary (0: <=50K, 1: >50K) and rename it to \"target\" for easier reference later."
   ],
   "id": "4c3d379fed4cd986"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Adult Income Dataset\n",
    "df_adult = pd.read_csv(\"data/adult.csv\")\n",
    "\n",
    "# Replace \"?\" with NaN\n",
    "df_adult.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "# Drop existing missing values for \"Perfect\" baseline\n",
    "df_adult.dropna(inplace=True)\n",
    "\n",
    "# Remove unnecessary columns, such as \"fnlwgt\" (final weight) and \"educational-num\" (redundant with \"education\")\n",
    "df_adult.drop(columns=[\"fnlwgt\", \"educational-num\"], inplace=True)\n",
    "\n",
    "# Standardise target variable (income -> 0: <=50K, 1: >50K)\n",
    "df_adult[\"income\"] = df_adult[\"income\"].apply(lambda x: 0 if x == \"<=50K\" else 1)\n",
    "\n",
    "# Store the target variable (\"income\") in a new column \"target\" for easier reference later\n",
    "df_adult.rename(columns={\"income\": \"target\"}, inplace=True)\n",
    "\n",
    "print(f\"Dataset has {len(df_adult)} rows\")\n",
    "print(df_adult.isnull().sum())"
   ],
   "id": "598b1bd8b8a978f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clean and Load COMPAS Dataset\n",
    "When cleaning the COMPAS Dataset, filter the rows based on [ProPublica\"s logic](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) to ensure consistency. Also, this study will focus only on two racial groups: \"African-American\" and \"Caucasian\". Make sure to drop unnecessary columns to simplify the analysis. Finally, rename the target variable \"two_year_recid\" to \"target\" for easier reference later."
   ],
   "id": "6843274f13359ce0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load COMPAS Dataset\n",
    "df_compas = pd.read_csv(\"data/compas-scores-two-years.csv\")\n",
    "\n",
    "# Filter rows, following ProPublica\"s logic (available at their GitHub (where the dataset is hosted) and https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n",
    "df_compas = df_compas[\n",
    "    (df_compas[\"days_b_screening_arrest\"] <= 30) &\n",
    "    (df_compas[\"days_b_screening_arrest\"] >= -30) &\n",
    "    (df_compas[\"is_recid\"] != -1) &\n",
    "    (df_compas[\"c_charge_degree\"] != \"O\") &\n",
    "    (df_compas[\"score_text\"] != \"N/A\")\n",
    "]\n",
    "\n",
    "# Filter Race to only account for 2 groups: African-American and Caucasian\n",
    "df_compas = df_compas[df_compas[\"race\"].isin([\"African-American\", \"Caucasian\"])]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "\"\"\"\n",
    "I will keep only the following columns:\n",
    "- two_year_recid (target variable)\n",
    "- age\n",
    "- c_charge_degree (current charge degree: Misdemeanor or Felony)\n",
    "- race (sensitive attribute)\n",
    "- sex (also a sensitive attribute)\n",
    "- priors_count (number of prior offenses)\n",
    "- juv_fel_count (number of juvenile felony offenses)\n",
    "- juv_misd_count (number of juvenile misdemeanor offenses)\n",
    "- juv_other_count (number of other juvenile offenses)\n",
    "All other columns will be dropped to simplify the analysis.\n",
    "\"\"\"\n",
    "columns_to_keep = [\n",
    "    \"two_year_recid\",\n",
    "    \"age\",\n",
    "    \"c_charge_degree\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"priors_count\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "]\n",
    "df_compas = df_compas[columns_to_keep]\n",
    "\n",
    "# Store the target variable (\"two_year_recid\") in a new column \"target\" for easier reference later\n",
    "df_compas.rename(columns={\"two_year_recid\": \"target\"}, inplace=True)\n",
    "\n",
    "print(f\"Dataset has {len(df_compas)} rows\")\n",
    "print(df_compas.isnull().sum())"
   ],
   "id": "835d99dfb87fc046",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clean and Load German Credit Dataset\n",
    "When cleaning the German Credit Dataset, remove the unnecessary index column. Handle missing values in \"Saving accounts\" and \"Checking account\" by replacing NaN with \"no_account\" to reflect the distinction between not having an account and not having the information. Standardise the target variable \"Risk\" to be binary (0: bad, 1: good) and rename it to \"target\" for easier reference later."
   ],
   "id": "2d6ef25f0ebc56d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Load German Credit Dataset\n",
    "df_german = pd.read_csv(\"data/german_credit_data.csv\")\n",
    "\n",
    "# Drop index column\n",
    "df_german.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "# In this dataset, NaN values in \"Saving accounts\" and \"Checking account\" indicate that no account exists, which is different from not having the information.\n",
    "# Therefore, I will replace NaN with \"no_account\" to reflect this distinction.\n",
    "df_german[\"Saving accounts\"] = df_german[\"Saving accounts\"].fillna(\"no_account\")\n",
    "df_german[\"Checking account\"] = df_german[\"Checking account\"].fillna(\"no_account\")\n",
    "\n",
    "# Standardise target variable (Risk -> 0: bad, 1: good)\n",
    "df_german[\"Risk\"] = df_german[\"Risk\"].apply(lambda x: 0 if x == \"bad\" else 1)\n",
    "\n",
    "# Store the target variable (\"Risk\") in a new column \"target\" for easier reference later\n",
    "df_german.rename(columns={\"Risk\": \"target\"}, inplace=True)\n",
    "\n",
    "print(f\"Dataset has {len(df_german)} rows\")\n",
    "\n",
    "print(df_german.isnull().sum())"
   ],
   "id": "cdf56e199bc5c134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function for Targeted EDA\n",
    "This function creates three plots side by side for a given dataset:\n",
    "- Target Distribution (0 vs 1)\n",
    "- Sensitive Group Distribution\n",
    "- Target Probability by Sensitive Group\n",
    "\n",
    "With this function, the baseline fairness of each dataset can be assessed before proceeding with further analysis."
   ],
   "id": "fe3d421756b6fc5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_targeted_eda(df, sensitive_column, dataset_name, outcome_label):\n",
    "    print(f\"Analysing {dataset_name}\")\n",
    "\n",
    "    # Create a 1x3 grid of plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    total_count = len(df)\n",
    "\n",
    "    def add_percentage_labels(ax, total):\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height()\n",
    "\n",
    "            if total > 0:\n",
    "                pct = 100 * height / total\n",
    "                ax.annotate(f\"{int(height)}\\n({pct:.1f}%)\",\n",
    "                            (p.get_x() + p.get_width() / 2., height),\n",
    "                            ha=\"center\", va=\"bottom\", fontsize=10, color=\"black\", xytext=(0, 5),\n",
    "                            textcoords=\"offset points\")\n",
    "\n",
    "    # First plot: Target Distribution\n",
    "    sns.countplot(x=\"target\", data=df, ax=axes[0], palette=\"viridis\", hue=\"target\", legend=False)\n",
    "    axes[0].set_title(f\"Target Distribution (0 vs 1)\")\n",
    "    axes[0].set_ylim(0, df[\"target\"].value_counts().max() * 1.15)\n",
    "    add_percentage_labels(axes[0], total_count)\n",
    "\n",
    "    # Second plot: Sensitive Group Distribution\n",
    "    sns.countplot(x=sensitive_column, data=df, ax=axes[1], palette=\"magma\", hue=sensitive_column, legend=False)\n",
    "    axes[1].set_title(f\"Sensitive Group Size: {sensitive_column.capitalize()}\")\n",
    "    axes[1].set_ylim(0, df[sensitive_column].value_counts().max() * 1.15)\n",
    "    add_percentage_labels(axes[1], total_count)\n",
    "\n",
    "    # Third plot: Target Probability by Sensitive Group\n",
    "    sns.barplot(x=sensitive_column, y=\"target\", data=df, errorbar=None, ax=axes[2], palette=\"coolwarm\", hue=sensitive_column, legend=False)\n",
    "\n",
    "    for p in axes[2].patches:\n",
    "        height = p.get_height()\n",
    "        axes[2].annotate(f\"{height:.2f}\",\n",
    "                        (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=10, xytext=(0, 5),\n",
    "                        textcoords=\"offset points\")\n",
    "\n",
    "    axes[2].set_title(f\"{outcome_label} Probability by {sensitive_column.capitalize()}\")\n",
    "    axes[2].set_ylabel(\"Probability (Mean Target)\")\n",
    "    axes[2].set_ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Get baseline fairness statistics\n",
    "    # Get the average value of the target variable for each sensitive group\n",
    "    group_means = df.groupby(sensitive_column)[\"target\"].mean()\n",
    "    print(f\"Probability of Positive Class per sensitive group:\")\n",
    "    for gender, percentage in group_means.items():\n",
    "        print(f\"{gender}: {(percentage * 100):.2f}%\")\n",
    "\n",
    "    # Calculate the gap between both groups (closer to 0 is better)\n",
    "    gap = (group_means.max() - group_means.min()).round(2)\n",
    "    baseline_bias = \"The data is already biased\" if gap > 0.10 else \"The data is relatively balanced\"\n",
    "    print(f\"Gap between groups: {gap} -> {baseline_bias}\\n\")"
   ],
   "id": "e28afab5fe88ada6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Adult Income (Sensitive: Sex or Race)\n",
    "run_targeted_eda(df_adult, sensitive_column=\"gender\", dataset_name=\"Adult Income\", outcome_label=\"High Income (>50K)\")\n",
    "\n",
    "# 2. COMPAS (Sensitive: Race)\n",
    "run_targeted_eda(df_compas, sensitive_column=\"race\", dataset_name=\"COMPAS Recidivism\", outcome_label=\"Recidivism within 2 Years\")\n",
    "\n",
    "# 3. German Credit (Sensitive: Sex)\n",
    "run_targeted_eda(df_german, sensitive_column=\"Sex\", dataset_name=\"German Credit\", outcome_label=\"Good Credit\")"
   ],
   "id": "8f880c3210ccdb6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ordinal Encoding Pre-Split\n",
    "This function encodes the categorical columns of the dataset, converting strings into integer representations. This transformation is necessary for imputation algorithms such as KNN (distance-based) and MICE (regression-based), which need numerical input vectors to compute missing values.\n",
    "The sensitive column follows a simple label encoding, converting categories into integers based on alphanumerical order. For the sensitive attributes in all of the datasets that were analysed, it resulted in the unprivileged group being mapped to 0 and the privileged group to 1."
   ],
   "id": "9e4b8a0fe888aa3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pre_split_encoding(df, target_column, sensitive_column, dataset_name):\n",
    "    print(f\"Ordinal encoding for {dataset_name}\")\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    # Extract categorical columns\n",
    "    categorical_columns = df_encoded.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    # Exclude target and sensitive columns from encoding\n",
    "    if target_column in categorical_columns: categorical_columns.remove(target_column)\n",
    "    if sensitive_column in categorical_columns: categorical_columns.remove(sensitive_column)\n",
    "\n",
    "    # Apply Ordinal Encoding\n",
    "    ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[categorical_columns] = ordinal_encoder.fit_transform(df_encoded[categorical_columns])\n",
    "\n",
    "    # Handle sensitive column (ensure 0 and 1 encoding) using label encoding\n",
    "    sensitive_label_encoder = LabelEncoder()\n",
    "    df_encoded[sensitive_column] = sensitive_label_encoder.fit_transform(df_encoded[sensitive_column])\n",
    "\n",
    "    # Print the mapping for sensitive attribute\n",
    "    mapping = dict(zip(sensitive_label_encoder.classes_, sensitive_label_encoder.transform(sensitive_label_encoder.classes_)))\n",
    "    # Dictionary comprehension to display numpy.int64 values as native ints for readability\n",
    "    print(f\"Sensitive Attribute \\\"{sensitive_column}\\\" Mapping: { {k: int(v) for k, v in mapping.items()} }\")\n",
    "\n",
    "    # Return the encoded dataset\n",
    "    return df_encoded"
   ],
   "id": "c19fec451db7a86c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Adult Income\n",
    "df_adult_encoded = pre_split_encoding(df_adult, target_column=\"target\", sensitive_column=\"gender\", dataset_name=\"Adult Income\")\n",
    "\n",
    "# 2. COMPAS\n",
    "df_compas_encoded = pre_split_encoding(df_compas, target_column=\"target\", sensitive_column=\"race\", dataset_name=\"COMPAS Recidivism\")\n",
    "\n",
    "# 3. German Credit\n",
    "df_german_encoded = pre_split_encoding(df_german, target_column=\"target\", sensitive_column=\"Sex\", dataset_name=\"German Credit\")"
   ],
   "id": "91984edff0b3f94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Correlation Matrices\n",
    "This function computes the pairwise Pearson correlation coefficients between all numerical features of the given dataset. It is important to:\n",
    "- **Detect multicollinearity**: Helps identify features that are highly correlated with each other;\n",
    "- **Identify Proxies**: This is the most important aspect, as it can reveal possible proxy variables, for example, non-sensitive features that are strongly correlated with the sensitive attributes. For example, as observed in the Adult Income correlation matrix, \"sex\" and \"relationship\" are strongly correlated (although negatively, likely due to encoding), which is expected, since some \"relationship\" contains values that are directly associated with values of the \"gender\" column (i.e., Husband (\"relationship\") -> Male (\"gender\") and Wife (\"relationship\") -> Female (\"gender\"). This shows that \"relationship\" acts as a direct proxy for \"gender\", and simply removing the \"gender\" column would not be sufficient to make the model \"blind\" to this attribute. Although both features are kept, it is important to keep this in mind, since an approach like \"Fairness Through Unawareness\" would likely fail here, as the model could recover gender information by using the \"relationship\" column."
   ],
   "id": "2aeb1bbbcbfc949"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_correlation_matrix(dataset, dataset_name):\n",
    "    df_corr = dataset.copy()\n",
    "\n",
    "    for col in df_corr.columns:\n",
    "        # Check if column is Object (string) or Category\n",
    "        if df_corr[col].dtype == \"object\" or df_corr[col].dtype.name == \"category\":\n",
    "            # Factorize to have numerical values for the correlation calculation\n",
    "            df_corr[col], _ = pd.factorize(df_corr[col])\n",
    "\n",
    "    # Compute the correlation\n",
    "    corr = df_corr.corr(method=\"spearman\")\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    # Show only one triangle as opposed to the entire square\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        mask=mask,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        vmax=1.0, vmin=-1.0,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        square=True,\n",
    "        linewidths=.5,\n",
    "        cbar_kws={\"shrink\": .5},\n",
    "        annot_kws={\"size\": 9}\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Feature Correlation: {dataset_name}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "37f4ecbc72872800",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Adult Income\n",
    "plot_correlation_matrix(df_adult_encoded, \"Adult\")\n",
    "\n",
    "# 2. COMPAS\n",
    "plot_correlation_matrix(df_compas_encoded, \"COMPAS\")\n",
    "\n",
    "# 3. German Credit\n",
    "plot_correlation_matrix(df_german_encoded, \"German Credit\")"
   ],
   "id": "887f21be12d2f6bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train-Test Split (80-20 Stratified)\n",
    "This function splits the data into train and test sets (80% train, 20% test), in a stratified manner. This ensures that the percentage of each of the classes is the same (or very close, if no perfect division is possible) in both train and test sets (i.e., both test and train sets have 70% of class 1 and 30% of class 0)."
   ],
   "id": "4a1ec9a7a816ee7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def stratified_train_test_split(df, target_column, dataset_name, random_state=42):\n",
    "    print(f\"Stratified splitting for {dataset_name}\")\n",
    "\n",
    "    # Separate target column from other columns\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Stratified Train-Test Split (80-20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Analyse overall distribution and sizes of train and test sets\n",
    "    print(f\"Train Shape: {X_train.shape} | Test Shape: {X_test.shape}\")\n",
    "    train_abs_val, train_rel_pct = y_train.value_counts().values, y_train.value_counts(normalize=True).values.round(4)\n",
    "    test_abs_val, test_rel_pct = y_test.value_counts().values, y_test.value_counts(normalize=True).values.round(4)\n",
    "    print(f\"Train Target Counts: {train_abs_val} ({train_rel_pct*100})\")\n",
    "    print(f\"Test Target Counts: {test_abs_val} ({test_rel_pct*100})\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "id": "ab4426375e9e39ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dictionary to hold all datasets\" splits\n",
    "datasets_splits = {\"Adult\": {}, \"COMPAS\": {}, \"German\": {}}\n",
    "\n",
    "# 1. Adult Income\n",
    "datasets_splits[\"Adult\"][\"X_train\"], datasets_splits[\"Adult\"][\"X_test\"], datasets_splits[\"Adult\"][\"y_train\"], datasets_splits[\"Adult\"][\"y_test\"] = stratified_train_test_split(df_adult_encoded, target_column=\"target\", dataset_name=\"Adult Income\")\n",
    "\n",
    "# 2. COMPAS\n",
    "datasets_splits[\"COMPAS\"][\"X_train\"], datasets_splits[\"COMPAS\"][\"X_test\"], datasets_splits[\"COMPAS\"][\"y_train\"], datasets_splits[\"COMPAS\"][\"y_test\"] = stratified_train_test_split(df_compas_encoded, target_column=\"target\", dataset_name=\"COMPAS Recidivism\")\n",
    "\n",
    "# 3. German Credit\n",
    "datasets_splits[\"German\"][\"X_train\"], datasets_splits[\"German\"][\"X_test\"], datasets_splits[\"German\"][\"y_train\"], datasets_splits[\"German\"][\"y_test\"] = stratified_train_test_split(df_german_encoded, target_column=\"target\", dataset_name=\"German Credit\")"
   ],
   "id": "36863502002de48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function to evaluate Fairness\n",
    "Given the actual classes and the predictions made by the model, this function computes both the Demographic Parity and the Equal Opportunity. After, it calculates the gap between both sensitive groups. These results are added to the model metrics, so that the final model performance (combination of both numerical performance and fairness performance) can be assessed."
   ],
   "id": "31879440ff044b99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_fairness_metrics(y_true, y_pred, sensitive_features):\n",
    "    df_eval = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred, \"sensitive\": sensitive_features})\n",
    "\n",
    "    # Demographic Parity\n",
    "    selection_rates = df_eval.groupby(\"sensitive\")[\"y_pred\"].mean()\n",
    "    dp_gap = selection_rates.max() - selection_rates.min()\n",
    "\n",
    "    # Equal Opportunity\n",
    "    tpr_df = df_eval[df_eval[\"y_true\"] == 1]\n",
    "    tpr_rates = tpr_df.groupby(\"sensitive\")[\"y_pred\"].mean()\n",
    "    eo_gap = tpr_rates.max() - tpr_rates.min()\n",
    "\n",
    "    return {\"DP Gap\": dp_gap, \"EO Gap\": eo_gap}"
   ],
   "id": "5a5371608391ee94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function to Train Models & Evaluate Performance + Fairness\n",
    "This function is abstracted to be reusable for different datasets, models, and configurations:\n",
    "- It starts by choosing the model passed as an argument and an appropriate scaler - Both Logistic Regression and MLP need to have the values scaled. XGBoost doesn't use any scaling - although it doesn't hurt its performance, it would be a \"waste\" of computation to scale values when it isn't necessary, so its natural behaviour was preserved.\n",
    "- Then, the values in numerical columns are scaled and the values in categorical columns are One-Hot Encoded.\n",
    "- Both the preprocessing and the classifier are encapsulated into a single Pipeline object. This prevents data leakage by ensuring that all transformations (such as scaling and encoding) are learned exclusively from the training set and subsequently applied to transform the test set during inference.\n",
    "- Finally, Performance metrics (F1 score, Accuracy) and Fairness metrics (as described on the get_fairness_metrics function) are stored, alongside the Selection Rate, which is the average class picked by the model across all predictions.\n",
    "    - This added metric is helpful to later detect and exclude trivial models that predict the same class for nearly all instances (i.e., picking 1 for every test case). In the case of the German Credit Dataset, where 70% of the values are of class 1, a lazy model could achieve an accuracy of 70% by simply predicting class 1 for everyone. The Selection Rate in this case would be close to 1, and with it we can identify that the model failed to learn meaningful patterns."
   ],
   "id": "b16e41fd3f81abd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_model(X_train, X_test, y_train, y_test, cat_columns, sensitive_column, experiment_name, model_type, random_state=42):\n",
    "    print(f\"\\n Training {model_type.upper()}: {experiment_name}\")\n",
    "\n",
    "    # Choose the correct model and scaling method\n",
    "    if model_type == \"logistic\":\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "        scaler = StandardScaler()\n",
    "    elif model_type == \"mlp\":\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, early_stopping=True, random_state=random_state)\n",
    "        scaler = StandardScaler()\n",
    "    elif model_type == \"xgboost\":\n",
    "        clf = XGBClassifier(eval_metric=\"logloss\", random_state=random_state)\n",
    "        scaler = \"passthrough\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    # Select the numerical columns to be scaled\n",
    "    num_columns = [c for c in X_train.columns if c not in cat_columns]\n",
    "\n",
    "    # Scale numerical columns (except when model is XGBoost) and One-Hot Encode categorical ones\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", scaler, num_columns),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the pipeline with both the preprocessing and the model (classifier) to be used\n",
    "    model = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    sensitive_test = X_test[sensitive_column]\n",
    "    fairness = get_fairness_metrics(y_test, y_pred, sensitive_test)\n",
    "\n",
    "    print(f\"Performance: Accuracy={acc:.4f} | F1={f1:.4f}\")\n",
    "    print(f\"Fairness:    DP Gap={fairness[\"DP Gap\"]:.4f} | EO Gap={fairness[\"EO Gap\"]:.4f}\")\n",
    "\n",
    "    # Return model metrics (Performance, Fairness and Selection Rate)\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1\": f1,\n",
    "        \"DP Gap\": fairness[\"DP Gap\"],\n",
    "        \"EO Gap\": fairness[\"EO Gap\"],\n",
    "        \"Selection Rate\": np.mean(y_pred)\n",
    "    }"
   ],
   "id": "cddf6cb5bbeda4eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Categorical columns of each dataset to feed to training loop\n",
    "adult_categorical = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"native-country\", \"gender\"]\n",
    "compas_categorical = [\"c_charge_degree\", \"sex\", \"race\"]\n",
    "german_categorical = [\"Housing\", \"Saving accounts\", \"Checking account\", \"Purpose\", \"Sex\"]\n",
    "\n",
    "# Models that will be trained\n",
    "models_to_run = [\"logistic\", \"xgboost\", \"mlp\"]\n",
    "\n",
    "# List to store results of all trained models (baseline)\n",
    "results = []\n",
    "\n",
    "for dataset_name, data in datasets_splits.items():\n",
    "    if dataset_name == \"Adult\":\n",
    "        categorical_columns = adult_categorical\n",
    "        sensitive_column = \"gender\"\n",
    "    elif dataset_name == \"COMPAS\":\n",
    "        categorical_columns = compas_categorical\n",
    "        sensitive_column = \"race\"\n",
    "    elif dataset_name == \"German\":\n",
    "        categorical_columns = german_categorical\n",
    "        sensitive_column = \"Sex\"\n",
    "    else: continue\n",
    "\n",
    "    for model_name in models_to_run:\n",
    "        metrics = run_model(\n",
    "            X_train=data[\"X_train\"],\n",
    "            X_test=data[\"X_test\"],\n",
    "            y_train=data[\"y_train\"],\n",
    "            y_test=data[\"y_test\"],\n",
    "            cat_columns=categorical_columns,\n",
    "            sensitive_column=sensitive_column,\n",
    "            experiment_name=f\"{dataset_name} Baseline\",\n",
    "            model_type=model_name\n",
    "        )\n",
    "\n",
    "        # Store the Dataset, Missing Data Pattern (in this case none -> Baseline), Imputation Method (in this case none) and Model Name\n",
    "        metrics.update({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Missing Pattern\": \"Baseline\",\n",
    "            \"Imputation Method\": \"None\",\n",
    "            \"Model\": model_name\n",
    "        })\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "# Print Results\n",
    "print(\"Baseline Results:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ],
   "id": "838d3b0bccc3bd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function to Introduce a specific Missing Data Pattern on the Data\n",
    "This functions allows to introduce a specific Missing Data Pattern on a chosen dataset. In this case, it will be used on both the training and test sets. This function is designed for 3 patterns: MCAR, MAR and Group-Based. The latter is a specific type of missing data relevant to this study, where the unprivileged group has a higher missing data rate than the privileged group. The random number generator (rng) is initialized at the beginning based on the random_state to ensure reproducibility, i.e., different runs will yield the same results.\n",
    "- MCAR: On the chosen target_column, missing values will appear at a missing_rate rate.\n",
    "- MAR: On the chosen target_column, the missing_rate increases as the values of the mar_cause_column increases, i.e., as age increases the probability of a missing value in target_column increases.\n",
    "- Group-Based: On the chosen target_column, missing values will appear at 1.5 times more than the base missing rate for the unprivileged group and 0.5 times less than the base missing rate for the privileged group."
   ],
   "id": "ae3fe896b111d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def introduce_missingness(df, target_column, pattern_type, missing_rate, sensitive_column, mar_cause_column=None, random_state=42):\n",
    "    df_missing = df.copy()\n",
    "    n = len(df)\n",
    "\n",
    "    # Initialize rng to ensure reproducibility\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # MCAR logic\n",
    "    if pattern_type == \"MCAR\":\n",
    "        n_missing = int(n * missing_rate)\n",
    "        missing_indices = rng.choice(df.index, size=n_missing, replace=False)\n",
    "        df_missing.loc[missing_indices, target_column] = np.nan\n",
    "\n",
    "    # MAR logic\n",
    "    elif pattern_type == \"MAR\":\n",
    "        # Get the percentiles of scores\n",
    "        percentiles = df[mar_cause_column].rank(pct=True)\n",
    "\n",
    "        # Get the weights (to later give more probability/weight to values on higher percentiles)\n",
    "        weights = percentiles ** 2\n",
    "        weights = weights / weights.sum()\n",
    "\n",
    "        n_missing = int(n * missing_rate)\n",
    "        missing_indices = rng.choice(df.index, size=n_missing, replace=False, p=weights)\n",
    "        df_missing.loc[missing_indices, target_column] = np.nan\n",
    "\n",
    "    # Group-Based logic\n",
    "    elif pattern_type == \"Group\":\n",
    "        # Unprivileged has a higher missing rate, Privileged has lower missing rate\n",
    "        unprivileged = missing_rate * 1.5\n",
    "        r1 = missing_rate * 0.5\n",
    "\n",
    "        # Get indices of Unprivileged group\n",
    "        unprivileged_indices = df[df[sensitive_column] == 0].index\n",
    "        n_missing_unprivileged = int(len(unprivileged_indices) * unprivileged)\n",
    "\n",
    "        # Introduce missing data on unprivileged group\n",
    "        missing_unprivileged_indices = rng.choice(unprivileged_indices, size=n_missing_unprivileged, replace=False)\n",
    "        df_missing.loc[missing_unprivileged_indices, target_column] = np.nan\n",
    "\n",
    "        # Get indices of Privileged group\n",
    "        privileged_indices = df[df[sensitive_column] == 1].index\n",
    "        n_missing_privileged = int(len(privileged_indices) * r1)\n",
    "\n",
    "        # Introduce missing data on privileged group\n",
    "        missing_privileged_indices = rng.choice(privileged_indices, size=n_missing_privileged, replace=False)\n",
    "        df_missing.loc[missing_privileged_indices, target_column] = np.nan\n",
    "\n",
    "    return df_missing"
   ],
   "id": "2a77e6cd54d0a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Dictionary that for each dataset stores:\n",
    "- The column where missing data will be introduced (target)\n",
    "- The sensitive column (sensitive_column)\n",
    "- The column where MAR will be based on (mar_cause)\n",
    "\"\"\"\n",
    "configs = {\n",
    "    \"Adult\": {\n",
    "        \"target\": \"education\",\n",
    "        \"sensitive_column\": \"gender\",\n",
    "        \"mar_cause\": \"age\"\n",
    "    },\n",
    "    \"COMPAS\": {\n",
    "        \"target\": \"priors_count\",\n",
    "        \"sensitive_column\": \"race\",\n",
    "        \"mar_cause\": \"age\"\n",
    "    },\n",
    "    \"German\": {\n",
    "        \"target\": \"Checking account\",\n",
    "        \"sensitive_column\": \"Sex\",\n",
    "        \"mar_cause\": \"Credit amount\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary that for each dataset stores all possible training sets with a specific missing data pattern\n",
    "missing_datasets = {}\n",
    "# Dictionary that for each dataset stores all possible test sets with a specific missing data pattern\n",
    "missing_test_sets = {}\n",
    "\n",
    "for name, params in configs.items():\n",
    "    print(f\"\\nGenerating missing dataset for {name}\")\n",
    "\n",
    "    # Initialize dictionary for this dataset\n",
    "    missing_datasets[name] = {}\n",
    "    missing_test_sets[name] = {}\n",
    "\n",
    "    # Get the CLEAN training data\n",
    "    original_train = datasets_splits[name][\"X_train\"].copy()\n",
    "    # Get the CLEAN test data (to replicate the missingness that exists in the training set)\n",
    "    original_test = datasets_splits[name][\"X_test\"].copy()\n",
    "\n",
    "    # 1. MCAR 15%\n",
    "    missing_datasets[name][\"MCAR_15\"] = introduce_missingness(\n",
    "        original_train, params[\"target\"], \"MCAR\", 0.15, params[\"sensitive_column\"]\n",
    "    )\n",
    "    missing_test_sets[name][\"MCAR_15\"] = introduce_missingness(\n",
    "        original_test, params[\"target\"], \"MCAR\", 0.15, params[\"sensitive_column\"]\n",
    "    )\n",
    "    print(f\"MCAR (15%) on \\\"{params[\"target\"]}\\\" generated.\")\n",
    "\n",
    "    # 2. MCAR 35%\n",
    "    missing_datasets[name][\"MCAR_35\"] = introduce_missingness(\n",
    "        original_train, params[\"target\"], \"MCAR\", 0.35, params[\"sensitive_column\"]\n",
    "    )\n",
    "    missing_test_sets[name][\"MCAR_35\"] = introduce_missingness(\n",
    "        original_test, params[\"target\"], \"MCAR\", 0.35, params[\"sensitive_column\"]\n",
    "    )\n",
    "    print(f\"MCAR (35%) on \\\"{params[\"target\"]}\\\" generated.\")\n",
    "\n",
    "    # 3. MAR 25%\n",
    "    missing_datasets[name][\"MAR\"] = introduce_missingness(\n",
    "        original_train, params[\"target\"], \"MAR\", 0.25, params[\"sensitive_column\"], mar_cause_column=params[\"mar_cause\"]\n",
    "    )\n",
    "    missing_test_sets[name][\"MAR\"] = introduce_missingness(\n",
    "        original_test, params[\"target\"], \"MAR\", 0.25, params[\"sensitive_column\"], mar_cause_column=params[\"mar_cause\"]\n",
    "    )\n",
    "    print(f\"MAR (25%) on \\\"{params[\"target\"]}\\\" (Dependent on \\\"{params[\"mar_cause\"]}\\\") generated.\")\n",
    "\n",
    "    # 4. Group Specific 25%\n",
    "    missing_datasets[name][\"Group\"] = introduce_missingness(\n",
    "        original_train, params[\"target\"], \"Group\", 0.25, params[\"sensitive_column\"]\n",
    "    )\n",
    "    missing_test_sets[name][\"Group\"] = introduce_missingness(\n",
    "        original_test, params[\"target\"], \"Group\", 0.25, params[\"sensitive_column\"]\n",
    "    )\n",
    "    print(f\"Group-Based (25%) on \\\"{params[\"target\"]}\\\" (Bias against \\\"{params[\"sensitive_column\"]}\\\") generated.\")\n",
    "\n",
    "print(\"Missing train and test sets generated.\")"
   ],
   "id": "2d275b99af70f876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if percentage of missingness is as expected (mainly for MCAR and MAR, as Group-Based depends on distribution, but should be close)\n",
    "for dataset_name, patterns in missing_datasets.items():\n",
    "    print(f\"\\n{\"=\"*10} {dataset_name} Missingness Check {\"=\"*10}\")\n",
    "    target_column = configs[dataset_name][\"target\"]\n",
    "    for pattern_name, df_missing in patterns.items():\n",
    "        missing_pct = df_missing[target_column].isnull().mean() * 100\n",
    "        missing_value = df_missing[target_column].isnull().sum()\n",
    "        total_size = df_missing.shape[0]\n",
    "        print(f\"Pattern: {pattern_name} | Missing in \\\"{target_column}\\\": {missing_value}/{total_size} ({missing_pct:.2f}%)\")"
   ],
   "id": "9b6643a01a1af063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualise Missingness Patterns\n",
    "This function, for a given dataset, shows up to 1000 rows (to make sure it is still readable, as too many columns would crowd the plot) of the data to allow for the effects of missing data generation on each of the cases.\n",
    "- MCAR (35%) - Should be able to see missing data \"randomly\" distributed across the rows\n",
    "- MAR - The data is sorted by the \"mar_cause\" column mentioned previously, to see that as the value in said column increases, the amount of missing data also increases\n",
    "- Group-Based - The data is sorted by the \"sensitive_column\" mentioned previously, and an horizontal line is drawn where the value in said column changes. The unprivileged group is drawn above the line, whereas the privileged group is drawn below the line. Should be able to see that the unprivileged group has a higher rate of missing data compared to the privileged group."
   ],
   "id": "af70f8fe97106722"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_missing_patterns(dataset, config_dict):\n",
    "    # Only visualise one of the MCAR patterns, as the other is quite similar\n",
    "    missing_patterns = [\"MCAR_35\", \"MAR\", \"Group\"]\n",
    "\n",
    "    for pattern in missing_patterns:\n",
    "        df_copy = dataset[pattern].copy()\n",
    "\n",
    "        # If dataset has over 1000 entries, sample it to allow for better visualization\n",
    "        if len(df_copy) > 1000:\n",
    "            # We use stratified sampling for Group, random for others\n",
    "            if pattern == \"Group\":\n",
    "                df_copy = df_copy.groupby(config_dict[\"sensitive_column\"], group_keys=False).sample(n=500, replace=False, random_state=42)\n",
    "            else:\n",
    "                df_copy = df_copy.sample(n=1000, random_state=42)\n",
    "            sample_size = \"(Sampled 1000 rows)\"\n",
    "        else:\n",
    "            sample_size = \"(All rows)\"\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        fig.patch.set_facecolor(\"white\")\n",
    "        ax.set_facecolor(\"white\")\n",
    "\n",
    "        # MCAR Pattern logic\n",
    "        if pattern == \"MCAR_35\":\n",
    "            msno.matrix(df_copy, ax=ax, sparkline=False, fontsize=10)\n",
    "            ax.set_title(f\"MCAR Pattern Visualization {sample_size}\", fontsize=16)\n",
    "\n",
    "        # MAR Pattern logic\n",
    "        elif pattern == \"MAR\":\n",
    "            # Retrieve the cause column\n",
    "            cause_column = config_dict[\"mar_cause\"]\n",
    "\n",
    "            # Sort by cause column (Low -> High)\n",
    "            df_copy = df_copy.sort_values(by=cause_column)\n",
    "\n",
    "            msno.matrix(df_copy, ax=ax, sparkline=False, fontsize=10)\n",
    "\n",
    "            # Add support text to the side\n",
    "            # Higher on the plot (lower values) it is cleaner\n",
    "            ax.text(x=len(df_copy.columns) + 0.5, y=len(df_copy)*0.1,\n",
    "                     s=f\"Low {cause_column}\\n(Cleaner)\",\n",
    "                     color=\"green\", va=\"center\")\n",
    "\n",
    "            # Lower on the plot (higher values) there's more missing data\n",
    "            ax.text(x=len(df_copy.columns) + 0.5, y=len(df_copy)*0.9,\n",
    "                     s=f\"High {cause_column}\\n(Missing)\",\n",
    "                     color=\"red\", va=\"center\")\n",
    "\n",
    "            ax.set_title(f\"MAR Pattern Visualization (Sorted by {cause_column}) {sample_size}\", fontsize=16)\n",
    "\n",
    "        # Group-Based Pattern logic\n",
    "        elif pattern == \"Group\":\n",
    "            # Retrieve the sensitive column\n",
    "            sensitive_column = config_dict[\"sensitive_column\"]\n",
    "\n",
    "            # Sort by sensitive group (0 on Top, 1 on Bottom)\n",
    "            df_copy = df_copy.sort_values(by=sensitive_column)\n",
    "\n",
    "            # Get the boundary between the groups\n",
    "            first_group_val = df_copy[sensitive_column].iloc[0]\n",
    "            boundary = len(df_copy[df_copy[sensitive_column] == first_group_val])\n",
    "\n",
    "            msno.matrix(df_copy, ax=ax, sparkline=False, fontsize=10)\n",
    "\n",
    "            # Add separating horizontal line\n",
    "            ax.axhline(y=boundary, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "            # Add label for Unprivileged group (Above the line)\n",
    "            ax.text(x=len(df_copy.columns) + 0.5, y=boundary/2,\n",
    "                     s=f\"Unprivileged (0)\\n(Higher Missingness)\",\n",
    "                     color=\"red\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "            # Add label for Privileged group (Below the line)\n",
    "            ax.text(x=len(df_copy.columns) + 0.5, y=boundary + (len(df_copy)-boundary)/2,\n",
    "                     s=f\"Privileged (1)\\n(Lower Missingness)\",\n",
    "                     color=\"green\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "            ax.set_title(f\"Group Pattern Visualization (Sorted by {sensitive_column}) {sample_size}\", fontsize=16)\n",
    "\n",
    "        plt.show()"
   ],
   "id": "aff5f160f01d6df0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Adult Income\n",
    "visualize_missing_patterns(missing_datasets[\"Adult\"], configs[\"Adult\"])\n",
    "\n",
    "# 2. COMPAS\n",
    "visualize_missing_patterns(missing_datasets[\"COMPAS\"], configs[\"COMPAS\"])\n",
    "\n",
    "# 3. German Credit\n",
    "visualize_missing_patterns(missing_datasets[\"German\"], configs[\"German\"])"
   ],
   "id": "4d4689c467146a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Imputation & Model Training Loop\n",
    "This section is responsible for iterating through every combination of Dataset, Missingness Pattern and Imputation Method to evaluate how different imputation strategies impact model performance and fairness.\n",
    "- The imputer is fitted exclusively on the training set to prevent leakage.\n",
    "- Then, the test set is transformed based on the values the imputer learned from the training set.\n",
    "- In the case of XGBoost, the No Imputation variant (XGBoost native handling) is also tested, where we just need to replace NaNs with a value like \"Missing\" so that the models treats missing data ans an informative feature.\n",
    "- Since KNN and MICE are mathematical methods, the values they return are continuous. Because a categorical class (such as \"relationship\") is made of integer values, the output of these imputation methods is rounded to the nearest class (i.e., 1.6 gets rounded to 2 and 1.2 gets rounded to 1).\n",
    "- After this preparation, the model is ready to be trained with the run_model function described previously."
   ],
   "id": "62974e5e729fe4ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imputation Methods\n",
    "imputers = {\n",
    "    \"Original (No Imputation)\": None,\n",
    "    \"Median\": SimpleImputer(strategy=\"median\"),\n",
    "    \"KNN\": KNNImputer(n_neighbors=5),\n",
    "    \"MICE\": IterativeImputer(max_iter=10, random_state=42)\n",
    "}\n",
    "\n",
    "for dataset_name, patterns_dict in missing_datasets.items():\n",
    "\n",
    "    # Get categorical columns, sensitive column and target (of missingness) column\n",
    "    sensitive_column, target_column = configs[dataset_name][\"sensitive_column\"], configs[dataset_name][\"target\"]\n",
    "    if dataset_name == \"Adult\": categorical_columns = adult_categorical\n",
    "    elif dataset_name == \"COMPAS\": categorical_columns = compas_categorical\n",
    "    elif dataset_name == \"German\": categorical_columns = german_categorical\n",
    "\n",
    "    print(f\"\\nCurrently Processing {dataset_name}:\")\n",
    "\n",
    "    for pattern_name, X_train_missing in patterns_dict.items():\n",
    "        print(f\"[Pattern: {pattern_name}]\")\n",
    "\n",
    "        for imp_name, imputer in imputers.items():\n",
    "            # Retrieve the corresponding Test Set with the same missingness pattern\n",
    "            X_test_missing = missing_test_sets[dataset_name][pattern_name].copy()\n",
    "\n",
    "            if imputer is None:\n",
    "                # No Imputation (XGBoost)\n",
    "                X_train_imputed = X_train_missing.copy()\n",
    "                X_test_imputed = X_test_missing.copy()\n",
    "\n",
    "                if target_column in categorical_columns:\n",
    "                    # Replace NaNs with \"Missing\" and convert to string to ensure compatibility\n",
    "                    X_train_imputed[target_column] = X_train_imputed[target_column].fillna(\"Missing\").astype(str)\n",
    "                    X_test_imputed[target_column] = X_test_imputed[target_column].fillna(\"Missing\").astype(str)\n",
    "\n",
    "            else:\n",
    "                # Fit Imputer on training set\n",
    "                X_train_imputed_array = imputer.fit_transform(X_train_missing)\n",
    "                X_train_imputed = pd.DataFrame(\n",
    "                    X_train_imputed_array,\n",
    "                    columns=X_train_missing.columns,\n",
    "                    index=X_train_missing.index\n",
    "                )\n",
    "\n",
    "                # Transform test set using the values the imputer fitted on train\n",
    "                X_test_imputed_array = imputer.transform(X_test_missing)\n",
    "                X_test_imputed = pd.DataFrame(\n",
    "                    X_test_imputed_array,\n",
    "                    columns=X_test_missing.columns,\n",
    "                    index=X_test_missing.index\n",
    "                )\n",
    "\n",
    "                # Round Categorical Columns\n",
    "                if target_column in categorical_columns:\n",
    "                    X_train_imputed[target_column] = X_train_imputed[target_column].round()\n",
    "                    X_test_imputed[target_column] = X_test_imputed[target_column].round()\n",
    "\n",
    "            # Train Models\n",
    "            for model_name in [\"logistic\", \"xgboost\", \"mlp\"]:\n",
    "\n",
    "                # Only XGBoost has native handling of NaNs, so skip other models\n",
    "                if imp_name == \"Original (No Imputation)\" and model_name != \"xgboost\":\n",
    "                    continue\n",
    "\n",
    "                y_train = datasets_splits[dataset_name][\"y_train\"]\n",
    "                y_test = datasets_splits[dataset_name][\"y_test\"]\n",
    "\n",
    "                # Train the model\n",
    "                metrics = run_model(\n",
    "                    X_train_imputed,\n",
    "                    X_test_imputed,\n",
    "                    y_train,\n",
    "                    y_test,\n",
    "                    cat_columns=categorical_columns,\n",
    "                    sensitive_column=sensitive_column,\n",
    "                    experiment_name=f\"{dataset_name} | {pattern_name} | {imp_name}\",\n",
    "                    model_type=model_name\n",
    "                )\n",
    "\n",
    "                metrics.update({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Missing Pattern\": pattern_name,\n",
    "                    \"Imputation Method\": imp_name,\n",
    "                    \"Model\": model_name\n",
    "                })\n",
    "\n",
    "                results.append(metrics)\n",
    "\n",
    "print(\"All models trained.\")\n",
    "\n",
    "# Print first 5 results just to check\n",
    "final_df = pd.DataFrame(results)\n",
    "print(final_df.head(5))"
   ],
   "id": "c138dbbb4b98151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save results to csv\n",
    "final_df.to_csv(\"results_full.csv\", index=False)"
   ],
   "id": "c600ef0fe7ae732a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results Visualisation\n",
    "This function reads the csv where all the results are stored and plots 9 graphs (3 for each dataset):\n",
    "- Fairness vs Performance Trade-off: This plot aims to show how different models and imputation methods, on average across the multiple missingness patterns, behave in terms of F1 Score and Demographic Parity Score. The further right the model is, the higher the F1 score, and the further down the model is the lower the gap between groups (Demographic Parity Gap). A \"perfect\" model would be situated in the bottom-right corner.\n",
    "- Impact of Imputation Methods on Fairness: This plot aims to show for each dataset, how each imputation method, on average across the multiple missingness patterns, affected the Demographic Parity Gap. For reference, the average Demographic Parity Gap achieved on the baseline models (where there was no missing data, therefore no imputation methods were performed) is shown as an horizontal line, to quickly distinguish methods that decreased or increased this gap.\n",
    "- How does an Increase in MCAR Severity Impact Performance?: This plot aims to show, as MCAR severity increases (starting at 0% - baseline, increasing to 15% and achieving a maximum of 35%), if models can maintain close to their baseline performance (on average) or if they suffer heavily from missing data, thus decreasing their F1 score."
   ],
   "id": "b95fca5797b15608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_df = pd.read_csv(\"results_full.csv\")\n",
    "\n",
    "def plot_results_analysis(df):\n",
    "\n",
    "    # Rename \"None\" Imputation in Baseline for \"Baseline\" for clarity\n",
    "    df.loc[df[\"Missing Pattern\"] == \"Baseline\", \"Imputation Method\"] = \"Baseline\"\n",
    "\n",
    "    # Plot 1: Fairness vs Performance Tradeoff\n",
    "\n",
    "    # Baseline points will be bigger compared to other models to be distinguishable\n",
    "    df[\"Shape Size\"] = df[\"Missing Pattern\"].apply(lambda x: \"Baseline\" if x == \"Baseline\" else \"Imputed\")\n",
    "\n",
    "    # Distinguish sizes between baseline and imputed models\n",
    "    size_dict = {\"Baseline\": 400, \"Imputed\": 100}\n",
    "\n",
    "    g = sns.relplot(\n",
    "        data=df,\n",
    "        x=\"F1\",\n",
    "        y=\"DP Gap\",\n",
    "        hue=\"Imputation Method\",\n",
    "        style=\"Model\",\n",
    "        size=\"Shape Size\",\n",
    "        sizes=size_dict,\n",
    "        col=\"Dataset\",\n",
    "        kind=\"scatter\",\n",
    "        palette=\"viridis\"\n",
    "    )\n",
    "\n",
    "    g.figure.suptitle(\"Fairness vs. Performance Trade-off\", y=1.05, fontsize=20)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Impact of Imputation Methods on Fairness\n",
    "\n",
    "    # Separate the dataframe into 2: Only with baseline results and without baseline results\n",
    "    df_baseline = df[df[\"Imputation Method\"] == \"Baseline\"]\n",
    "    df_missing = df[df[\"Imputation Method\"] != \"Baseline\"]\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=df_missing,\n",
    "        x=\"Missing Pattern\",\n",
    "        y=\"DP Gap\",\n",
    "        hue=\"Imputation Method\",\n",
    "        col=\"Dataset\",\n",
    "        col_wrap=1,\n",
    "        kind=\"bar\",\n",
    "        height=4,\n",
    "        aspect=3,\n",
    "        palette=\"rocket_r\",\n",
    "        edgecolor=\"black\",\n",
    "        errorbar=None,\n",
    "        legend_out=True\n",
    "    )\n",
    "\n",
    "    g.figure.suptitle(\"Impact of Imputation on Fairness (Red Line = Baseline Fairness)\", y=1.02, fontsize=20)\n",
    "\n",
    "    # Map the axes to the correct dataset names\n",
    "    axes = g.axes.flat\n",
    "    dataset_names = df_missing[\"Dataset\"].unique()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        # Get the dataset that is shown in this plot\n",
    "        current_dataset = dataset_names[i]\n",
    "\n",
    "        # Get the average Demographic Parity Gap across baseline plots for the current dataset\n",
    "        baseline_val = df_baseline[df_baseline[\"Dataset\"] == current_dataset][\"DP Gap\"].mean()\n",
    "\n",
    "        # Draw the line for visual comparison\n",
    "        if not pd.isna(baseline_val):\n",
    "            ax.axhline(baseline_val, color=\"red\", linestyle=\"--\", linewidth=2.5, label=\"Baseline\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: MCAR Severity Impact on Perfromance\n",
    "\n",
    "    # Filter only baseline results and MCAR patterns\n",
    "    mcar_df = df[df[\"Missing Pattern\"].isin([\"Baseline\", \"MCAR_15\", \"MCAR_35\"])].copy()\n",
    "    pattern_map = {\"Baseline\": 0, \"MCAR_15\": 15, \"MCAR_35\": 35}\n",
    "    mcar_df[\"Missing %\"] = mcar_df[\"Missing Pattern\"].map(pattern_map)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=mcar_df,\n",
    "        x=\"Missing %\",\n",
    "        y=\"F1\",\n",
    "        hue=\"Dataset\",\n",
    "        style=\"Model\",\n",
    "        markers=True,\n",
    "        markersize=10,\n",
    "        dashes=False,\n",
    "        linewidth=2,\n",
    "        palette=\"Set1\",\n",
    "        errorbar=None\n",
    "    )\n",
    "    plt.title(\"Performance Resilience: How datasets handle increased MCAR severity\", fontsize=18)\n",
    "    plt.xticks([0, 15, 35], [\"Baseline (0%)\", \"MCAR 15%\", \"MCAR 35%\"])\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.grid(True, which=\"major\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "83d8c560de05b309",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the results\n",
    "plot_results_analysis(final_df)"
   ],
   "id": "35b9bc99285c01b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Get the \"Best\" Models\n",
    "This function is designed to, for each dataset and missingness pattern, to get the best performing model. Every model is given a Model Score, which takes into account both F1 score and Demographic Parity Gap:\n",
    "\n",
    "$$\n",
    "\\text{Model Score} = F1 - 1.5 \\times \\text{DP Gap}\n",
    "$$\n",
    "\n",
    "This scoring method gives one and a half times (1.5) more importance to the Fairness Metric, to benefit models that managed to improve fairness although their performance may have decreased slightly. The value 1.5 was chosen to give an edge to the Fairness Metric, while still being careful to not \"destroy\" the performance metric for a slight fairness improvement.\n",
    "\n",
    "In the case multiple models have the same Model Score, the model with the lowest DP Gap, that is, the model that is fairer gets chosen."
   ],
   "id": "41ac71153b9fdb4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_best_models(df, lambda_val=1.5):\n",
    "    # Calculate Model Score based on the formula described above\n",
    "    df_scored = df.copy()\n",
    "    df_scored[\"Model Score\"] = df_scored[\"F1\"] - (lambda_val * df_scored[\"DP Gap\"])\n",
    "\n",
    "    best_models = []\n",
    "    datasets = df_scored[\"Dataset\"].unique()\n",
    "    patterns = df_scored[\"Missing Pattern\"].unique()\n",
    "\n",
    "    for ds in datasets:\n",
    "        print(f\"Analysing {ds}\")\n",
    "        for pat in patterns:\n",
    "            subset = df_scored[(df_scored[\"Dataset\"] == ds) & (df_scored[\"Missing Pattern\"] == pat)]\n",
    "\n",
    "            # Find row with Max Model Score. In case of draws, chose the one with the lowest DP Gap\n",
    "            best_run = subset.sort_values(by=[\"Model Score\", \"DP Gap\"], ascending=[False, True]).iloc[0]\n",
    "\n",
    "            # Only add imputation method if the current pattern isn't baseline (it would be redundant otherwise)\n",
    "            imputation_method = str(\"+ \" + best_run[\"Imputation Method\"] + \" \") if pat != \"Baseline\" else \"\"\n",
    "\n",
    "            print(f\"   [{pat}]: {best_run[\"Model\"]} {imputation_method}(Score: {best_run[\"Model Score\"]:.3f} | F1: {best_run[\"F1\"]:.3f}, DP: {best_run[\"DP Gap\"]:.3f})\")\n",
    "\n",
    "            best_models.append(best_run)\n",
    "\n",
    "    return pd.DataFrame(best_models)\n",
    "\n",
    "best_models = get_best_models(final_df)"
   ],
   "id": "1078372d803ac13f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Removing Trivial Models\n",
    "As described previously, the Selection Rate is a way of checking if a model actually learned meaningful patterns. This checks if there are any trivial models and, if so remove them. A model that was considered to be a \"best model\" (MLP baseline for the German Dataset) was actually a trivial model, that basically guessed the majority class (1) for close to all test cases, getting an accuracy close to the actual positive class percentage (69% accuracy vs 70% of positive cases in the target variable), and it achieved a Selection Rate of 0.99 which proves this claim.\n",
    "\n",
    "After removing these models, the plots and best models are recalculated."
   ],
   "id": "d44aa0fd332ff004"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A model is trivial if it predicts (almost) the same class for all test cases (either 0 or 1)\n",
    "# A threshold of 10% for each side was defined as the limit between a trivial and a non-trivial model\n",
    "valid_mask = (final_df[\"Selection Rate\"] > 0.10) & (final_df[\"Selection Rate\"] < 0.90)\n",
    "\n",
    "\n",
    "df_clean = final_df[valid_mask].copy()\n",
    "n_dropped = len(final_df) - len(df_clean)\n",
    "print(f\"Removed {n_dropped} out of {len(final_df)} models\")"
   ],
   "id": "7ebe9eb408853fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_results_analysis(df_clean)",
   "id": "f54832b2e4640c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_best_models = get_best_models(df_clean)",
   "id": "82dba4c28d77f51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Appendix\n",
    "This section aims to validate the results across 50 different random seeds to ensure robustness. The general process follows the same process fone in this notebook."
   ],
   "id": "cb0b5864cf92a7f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the seeds to be trained (50, selected this way to ensure reproducibility)\n",
    "SEEDS = list(range(42, 92))\n",
    "\n",
    "# Store the datasets to be used\n",
    "dataset_map = {\n",
    "    \"Adult\": df_adult_encoded,\n",
    "    \"COMPAS\": df_compas_encoded,\n",
    "    \"German\": df_german_encoded\n",
    "}\n",
    "\n",
    "# Define the imputers to be used\n",
    "imputers = {\n",
    "    \"Original (No Imputation)\": None,\n",
    "    \"Median\": SimpleImputer(strategy=\"median\"),\n",
    "    \"KNN\": KNNImputer(n_neighbors=5),\n",
    "    \"MICE\": IterativeImputer(max_iter=10) # random_state will be set in loop\n",
    "}\n",
    "\n",
    "multiple_validation_results = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"Running models for seed {seed}\")\n",
    "\n",
    "    # Split datasets for this seed\n",
    "    current_splits = {}\n",
    "    for ds_name, df_obj in dataset_map.items():\n",
    "        # Create a stratified train-test split\n",
    "        X_train, X_test, y_train, y_test = stratified_train_test_split(\n",
    "            df_obj, target_column=\"target\", dataset_name=ds_name, random_state=seed\n",
    "        )\n",
    "\n",
    "        current_splits[ds_name] = {\n",
    "            \"X_train\": X_train, \"X_test\": X_test,\n",
    "            \"y_train\": y_train, \"y_test\": y_test\n",
    "        }\n",
    "\n",
    "    # Iterate through datasets and configurations (sensitive column and mar_cause)\n",
    "    for dataset_name, params in configs.items():\n",
    "        if dataset_name == \"Adult\":\n",
    "            categorical_columns, sensitive_column, target = adult_categorical, \"gender\", \"education\"\n",
    "        elif dataset_name == \"COMPAS\":\n",
    "            categorical_columns, sensitive_column, target = compas_categorical, \"race\", \"priors_count\"\n",
    "        elif dataset_name == \"German\":\n",
    "            categorical_columns, sensitive_column, target = german_categorical, \"Sex\", \"Checking account\"\n",
    "\n",
    "        # Retrieve Seed Data\n",
    "        X_train_clean = current_splits[dataset_name][\"X_train\"]\n",
    "        X_test_clean = current_splits[dataset_name][\"X_test\"]\n",
    "        y_train = current_splits[dataset_name][\"y_train\"]\n",
    "        y_test = current_splits[dataset_name][\"y_test\"]\n",
    "\n",
    "        patterns = [\"Baseline\", \"MCAR_15\", \"MCAR_35\", \"MAR\", \"Group\"]\n",
    "        # Iterate through missingness patterns\n",
    "        for pattern in patterns:\n",
    "            # Generate Missing Data for this seed\n",
    "            if pattern == \"Baseline\":\n",
    "                # Baseline has no missing data, therefore also has no imputation\n",
    "                X_train_miss = X_train_clean.copy()\n",
    "                X_test_miss = X_test_clean.copy()\n",
    "\n",
    "                current_imputers = {\"Baseline\" : \"Baseline\"}\n",
    "            else:\n",
    "                # MCAR_15 and MCAR_35 have the missing rate shown on their name, MAR and Group-Based have a missing rate of 25%\n",
    "                rate = 0.15 if \"15\" in pattern else (0.35 if \"35\" in pattern else 0.25)\n",
    "                # Get the missing data pattern (for MCAR_15 and MCAR_35, the pattern is the same, so strip what comes before \"_\")\n",
    "                pat_type = pattern.split(\"_\")[0]\n",
    "\n",
    "                # Inject missing data on train set\n",
    "                X_train_miss = introduce_missingness(\n",
    "                    X_train_clean, target, pat_type, rate,\n",
    "                    params[\"sensitive_column\"], params.get(\"mar_cause\"), random_state=seed\n",
    "                )\n",
    "                # Inject missing data on test set\n",
    "                X_test_miss = introduce_missingness(\n",
    "                    X_test_clean, target, pat_type, rate,\n",
    "                    params[\"sensitive_column\"], params.get(\"mar_cause\"), random_state=seed\n",
    "                )\n",
    "\n",
    "                # Use the imputers defined initially\n",
    "                current_imputers = imputers\n",
    "\n",
    "            # Iterate through the imputers\n",
    "            for imp_name, imp_obj in current_imputers.items():\n",
    "\n",
    "                # Set the seed (same as current) for MICE\n",
    "                if imp_name == \"MICE\":\n",
    "                    imputer = IterativeImputer(max_iter=10, random_state=seed)\n",
    "                # XGBoost native handling\n",
    "                elif imp_obj is None:\n",
    "                    imputer = None\n",
    "                else:\n",
    "                    imputer = imp_obj\n",
    "\n",
    "                # Baseline has no imputation, so \"imputed\" train and test sest are the same as the original\n",
    "                if imputer == \"Baseline\":\n",
    "                    imp_name = \"None\"\n",
    "                    X_train_imp = X_train_miss.copy()\n",
    "                    X_test_imp = X_test_miss.copy()\n",
    "\n",
    "                # XGBoost native handling, similar handling as \"Imputation & Model Training Loop\" section of the notebook\n",
    "                elif imputer is None:\n",
    "                    X_train_imp = X_train_miss.copy()\n",
    "                    X_test_imp = X_test_miss.copy()\n",
    "\n",
    "                    if target in categorical_columns:\n",
    "                        X_train_imp[target] = X_train_imp[target].fillna(\"Missing\").astype(str)\n",
    "                        X_test_imp[target] = X_test_imp[target].fillna(\"Missing\").astype(str)\n",
    "                else:\n",
    "                    try:\n",
    "                        # Fit on training set\n",
    "                        X_train_imp_vals = imputer.fit_transform(X_train_miss)\n",
    "                        X_train_imp = pd.DataFrame(X_train_imp_vals, columns=X_train_miss.columns, index=X_train_miss.index)\n",
    "\n",
    "                        # Transform on test set\n",
    "                        X_test_imp_vals = imputer.transform(X_test_miss)\n",
    "                        X_test_imp = pd.DataFrame(X_test_imp_vals, columns=X_test_miss.columns, index=X_test_miss.index)\n",
    "\n",
    "                        # Round values in categorical columns to closest integer\n",
    "                        if target in categorical_columns:\n",
    "                            X_train_imp[target] = X_train_imp[target].round()\n",
    "                            X_test_imp[target] = X_test_imp[target].round()\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Skipping {imp_name} on {dataset_name} seed {seed} due to error: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # Train the models\n",
    "                for model_name in [\"logistic\", \"xgboost\", \"mlp\"]:\n",
    "\n",
    "                    # Only XGBoost can be trained with no imputation for missing data\n",
    "                    if imputer is None and model_name != \"xgboost\":\n",
    "                        continue\n",
    "\n",
    "                    # Train model\n",
    "                    metrics = run_model(\n",
    "                        X_train_imp,\n",
    "                        X_test_imp,\n",
    "                        y_train,\n",
    "                        y_test,\n",
    "                        categorical_columns,\n",
    "                        sensitive_column,\n",
    "                        f\"{dataset_name}_{pattern}_{imp_name}\",\n",
    "                        model_type=model_name,\n",
    "                        random_state=seed\n",
    "                    )\n",
    "\n",
    "                    # Add Metadata\n",
    "                    metrics.update({\n",
    "                        \"Dataset\": dataset_name,\n",
    "                        \"Missing Pattern\": pattern,\n",
    "                        \"Imputation Method\": imp_name,\n",
    "                        \"Model\": model_name,\n",
    "                        \"Seed\": seed\n",
    "                    })\n",
    "\n",
    "                    multiple_validation_results.append(metrics)\n",
    "\n",
    "# Save results to dataframe\n",
    "df_results = pd.DataFrame(multiple_validation_results)\n",
    "\n",
    "# Filter out trivial models (same threshold as before)\n",
    "df_results = df_results[(df_results[\"Selection Rate\"] > 0.10) & (df_results[\"Selection Rate\"] < 0.90)].copy()\n",
    "\n",
    "# Calculate Model Score (as described previously)\n",
    "df_results[\"Model Score\"] = df_results[\"F1\"] - (1.5 * df_results[\"DP Gap\"])\n",
    "\n",
    "# Store raw results\n",
    "df_results.to_csv(\"multiple_validation_results_raw.csv\")\n",
    "\n",
    "# For each dataset, Missingness Pattern, Imputation Method and Model calculate the average and standard deviation of F1, DP Gap and Model Score across all seeds\n",
    "df_results = df_results.groupby([\"Dataset\", \"Missing Pattern\", \"Imputation Method\", \"Model\"])[\n",
    "    [\"F1\", \"DP Gap\", \"Model Score\"]\n",
    "].agg([\"mean\", \"std\"])\n",
    "\n",
    "df_results.columns = [f\"{col[0]} ({col[1]})\" for col in df_results.columns]\n",
    "\n",
    "df_results = df_results.reset_index()\n",
    "\n",
    "# Store final results to a new csv\n",
    "df_results.to_csv(\"multiple_validation_results.csv\", index=False)"
   ],
   "id": "a678e37a8fb4af7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# T-Test to check if more complex imputers significantly outperform simpler ones\n",
    "This function aims to understand if the more complex imputation methods (KNN, MICE) significantly outperform the simpler Median imputation (or no Imputation for XGBoost) in terms of F1 Score and DP Gap across different datasets and missingness patterns, to understand if the added complexity is justified."
   ],
   "id": "be209a9c8a46257e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_automatic_significance(raw_csv_path=\"stability_results_raw.csv\"):\n",
    "    df = pd.read_csv(raw_csv_path)\n",
    "\n",
    "    # Exclude \"Baseline\" pattern\n",
    "    scenarios = df[df[\"Missing Pattern\"] != \"Baseline\"][[\"Dataset\", \"Missing Pattern\"]].drop_duplicates().values\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for ds, pat in scenarios:\n",
    "        # Filter relevant data\n",
    "        subset = df[(df[\"Dataset\"] == ds) & (df[\"Missing Pattern\"] == pat)]\n",
    "\n",
    "        # Get the highest scoring model\n",
    "        leaderboard = subset.groupby([\"Model\", \"Imputation Method\"])[\"Model Score\"].mean().reset_index()\n",
    "        winner_row = leaderboard.loc[leaderboard[\"Model Score\"].idxmax()]\n",
    "\n",
    "        winner_model = winner_row[\"Model\"]\n",
    "        winner_imp = winner_row[\"Imputation Method\"]\n",
    "\n",
    "        # Get baseline score (for XGBoost is native handling, otherwise is median)\n",
    "        if winner_model == \"xgboost\":\n",
    "            baseline_imp = \"Original (No Imputation)\"\n",
    "        else:\n",
    "            baseline_imp = \"Median\"\n",
    "\n",
    "        # Initialize Result Dictionary\n",
    "        result = {\n",
    "            \"Dataset\": ds,\n",
    "            \"Pattern\": pat,\n",
    "            \"Winner_Model\": winner_model,\n",
    "            \"Winner_Imputation\": winner_imp,\n",
    "            \"Baseline_Imputation\": baseline_imp,\n",
    "            \"Mean_Diff\": 0.0,\n",
    "            \"P_Value\": None,\n",
    "            \"Significant\": False,\n",
    "            \"Comment\": \"\"\n",
    "        }\n",
    "\n",
    "        # If the winner is the baseline imputation method, no t-test is needed\n",
    "        if winner_imp == baseline_imp:\n",
    "            print(f\"[{ds} | {pat}] Winner is Baseline ({baseline_imp}).\")\n",
    "            result[\"Comment\"] = \"Winner is Baseline (No test needed)\"\n",
    "            results.append(result)\n",
    "            continue\n",
    "\n",
    "        # 5. CASE B: Complex Imputation Won -> Run Stats\n",
    "\n",
    "        # Get Paired Seeds\n",
    "        winner_runs = subset[(subset[\"Model\"] == winner_model) & (subset[\"Imputation Method\"] == winner_imp)].set_index(\"Seed\")[\"Model Score\"]\n",
    "        baseline_runs = subset[(subset[\"Model\"] == winner_model) & (subset[\"Imputation Method\"] == baseline_imp)].set_index(\"Seed\")[\"Model Score\"]\n",
    "\n",
    "        # Align\n",
    "        aligned = pd.concat([winner_runs, baseline_runs], axis=1, join=\"inner\")\n",
    "        aligned.columns = [\"Winner\", \"Baseline\"]\n",
    "\n",
    "        # Run T-Test\n",
    "        t_stat, p_val = ttest_rel(aligned[\"Winner\"], aligned[\"Baseline\"])\n",
    "        mean_diff = aligned[\"Winner\"].mean() - aligned[\"Baseline\"].mean()\n",
    "\n",
    "        # Print the results of the test (at the end, \"Significant\" is printed if the p-value is lesser than 0.05, i.e., the complex imputation gave significant improvements compared to the baseline\n",
    "        sig_str = \"Significant\" if p_val < 0.05 else \"\"\n",
    "        print(f\"[{ds} | {pat}] Winner ({winner_imp}) vs {baseline_imp}: ={mean_diff:.3f}, p={p_val:.4f} {sig_str}\")\n",
    "\n",
    "        result[\"Mean_Diff\"] = mean_diff\n",
    "        result[\"P_Value\"] = p_val\n",
    "        result[\"Significant\"] = p_val < 0.05\n",
    "        result[\"Comment\"] = \"Significant\" if p_val < 0.05 else \"Not Significant\"\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "id": "aec4db657f93dac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_ttest = run_automatic_significance(raw_csv_path=\"multiple_validation_results_raw.csv\")\n",
    "df_ttest.to_csv(\"t_test_results.csv\", index=False)"
   ],
   "id": "2e39181b9c0bff0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Dive on MCAR (35%) on COMPAS Dataset\n",
    "We saw, on the base study performed on random_state=42, that Median imputation outperformed other methods on the COMPAS dataset with MCAR (35%) missingness pattern, but after the multiple seed validation, MICE imputation emerged as the winner (p=0.016706 < 0.05)."
   ],
   "id": "5660aef20788840c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "ds_name = \"COMPAS\"\n",
    "target_col_miss = \"priors_count\"\n",
    "\n",
    "# Get Data\n",
    "df_dive = df_compas_encoded.copy()\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(\n",
    "    df_dive, target_column=\"target\", dataset_name=ds_name, random_state=seed\n",
    ")\n",
    "\n",
    "# Inject MCAR 35%\n",
    "X_train_miss = introduce_missingness(X_train, target_col_miss, \"MCAR\", 0.35, \"race\", random_state=seed)\n",
    "\n",
    "# Calculate RMSE for both Median and MICE reconstructions\n",
    "median = SimpleImputer(strategy=\"median\")\n",
    "mice = IterativeImputer(max_iter=10, random_state=seed)\n",
    "\n",
    "X_median = pd.DataFrame(median.fit_transform(X_train_miss), columns=X_train.columns, index=X_train.index)\n",
    "X_mice = pd.DataFrame(mice.fit_transform(X_train_miss), columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "def get_rmse(X_true, X_imp, X_raw, col):\n",
    "    mask = X_raw[col].isna()\n",
    "    truth = X_true.loc[mask, col]\n",
    "    guess = X_imp.loc[mask, col]\n",
    "    return math.sqrt(mean_squared_error(truth, guess))\n",
    "\n",
    "rmse_med = get_rmse(X_train, X_median, X_train_miss, target_col_miss)\n",
    "rmse_mice = get_rmse(X_train, X_mice, X_train_miss, target_col_miss)\n",
    "\n",
    "print(f\"Column: {target_col_miss}\")\n",
    "print(f\"Median RMSE: {rmse_med:.4f} (Baseline)\")\n",
    "print(f\"MICE RMSE:   {rmse_mice:.4f} (Global Winner)\")\n",
    "print(f\"Percentual gain in fidelity: {((rmse_med - rmse_mice)/rmse_med)*100:.1f}%\")\n",
    "\n",
    "if rmse_mice < rmse_med:\n",
    "    print(\"MICE reconstructed the data more accurately, explaining its long-term stability.\")\n",
    "else:\n",
    "    print(\"MICE failed to reconstruct better on this seed (explains why it lost here).\")"
   ],
   "id": "6152fbdfe1ccdecc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
